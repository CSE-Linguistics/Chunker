{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM only POS tags.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLaGQaPeK_3B",
        "outputId": "4784aa30-ae3b-4833-c692-1c63ca261824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY8SSclOSmkD"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tags = [\"O\", \"B\", \"I\"]\n",
        "tag_to_index = {k:v for v,k in enumerate(tags)}\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ1v0QBTOxa-"
      },
      "source": [
        "path = \"/content/drive/My Drive/assignment2dataset/\"\n",
        "import os\n",
        "f = open(os.path.join(path,\"train.txt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7HjDCNmUsEX"
      },
      "source": [
        "# Chunking Using Bi-LSTM (only POS Tags)\n",
        "\n",
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwEg19jzY_69"
      },
      "source": [
        "def load_data_bilstm_pos_tags(file):\n",
        "\t# Opening file \n",
        "  f = open(file, 'r') \n",
        "  sents = []\n",
        "  sent = []\n",
        "  word_num = {}\n",
        "  word_num['^'] = 0\n",
        "  word_num['OOV'] = 1\n",
        "  count = 2\n",
        "  sents_tags = []\n",
        "  sent = [0]\n",
        "  sent_tags = [0]\n",
        "  for line in f:\n",
        "    cur_line = line.strip()\n",
        "    if not cur_line:\n",
        "      sents.append(sent)\n",
        "      sents_tags.append(sent_tags)\n",
        "      sent = [0]\n",
        "      sent_tags = [0]\n",
        "    else:\n",
        "      cur_line_list = cur_line.split(\" \")\n",
        "      key = tuple(cur_line_list[1])\n",
        "      if key not in word_num:\n",
        "          word_num[key] = count\n",
        "          count += 1\n",
        "      cur_line_list[2] = cur_line_list[2][0]\n",
        "      sent.append(word_num[key])\n",
        "      sent_tags.append(tag_to_index[cur_line_list[2]])\n",
        "\n",
        "  return sents, word_num,count, sents_tags\n",
        "sents, word_num,count, sents_tags = load_data_bilstm_pos_tags(os.path.join(path,\"train.txt\"))\n",
        "assert len(sents) == len(sents_tags)\n",
        "for i in range(len(sents)):\n",
        "  assert len(sents[i]) == len(sents_tags[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BBvh1G5gqY2"
      },
      "source": [
        "def one_hot(sequences, num_categories):\n",
        "\tone_hot_sequences = []\n",
        "\tfor s in sequences:\n",
        "\t\tone_hot_seq = []\n",
        "\t\tfor i in s:\n",
        "\t\t\tone_hot_seq.append(np.zeros(num_categories))\n",
        "\t\t\tone_hot_seq[-1][i] = 1.0\n",
        "\t\tone_hot_sequences.append(one_hot_seq)\n",
        "\treturn np.array(one_hot_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e93b7jvYNTuX"
      },
      "source": [
        "MAX_LEN = len(max(sents, key=len))\n",
        "def pad_sentences(sent_words_labelled, sent_tags_labelled,MAX_LEN):\n",
        "\tsent_words_labelled = pad_sequences(sent_words_labelled, maxlen = MAX_LEN, padding='post')\n",
        "\tsent_tags_labelled = pad_sequences(sent_tags_labelled, maxlen = MAX_LEN, padding='post')\n",
        "\treturn sent_words_labelled, sent_tags_labelled\n",
        "\n",
        "train_words, train_tags = pad_sentences(sents, sents_tags,MAX_LEN)\n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtwaCNqeeqBr"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "import nltk\n",
        "from sklearn.metrics import confusion_matrix\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(len(max(sents, key=len)), )))\n",
        "model.add(Embedding(count, 128))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tags))))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer=Nadam(0.001),metrics=['accuracy',ignore_class_accuracy(0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhp72vMJLCyR"
      },
      "source": [
        "train_tags = np.asarray(one_hot(train_tags, len(tags)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHeU6x7he7D2",
        "outputId": "a2c9362a-1294-451e-89de-37f7dac28d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(np.asarray(train_words),train_tags, batch_size=128, epochs=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.3722 - accuracy: 0.8193 - ignore_accuracy: 0.4958\n",
            "Epoch 2/100\n",
            "70/70 [==============================] - 119s 2s/step - loss: 0.1544 - accuracy: 0.9379 - ignore_accuracy: 0.7790\n",
            "Epoch 3/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0799 - accuracy: 0.9733 - ignore_accuracy: 0.9067\n",
            "Epoch 4/100\n",
            "70/70 [==============================] - 121s 2s/step - loss: 0.0698 - accuracy: 0.9766 - ignore_accuracy: 0.9190\n",
            "Epoch 5/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0658 - accuracy: 0.9778 - ignore_accuracy: 0.9231\n",
            "Epoch 6/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0625 - accuracy: 0.9788 - ignore_accuracy: 0.9269\n",
            "Epoch 7/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0592 - accuracy: 0.9801 - ignore_accuracy: 0.9317\n",
            "Epoch 8/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0549 - accuracy: 0.9821 - ignore_accuracy: 0.9392\n",
            "Epoch 9/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0512 - accuracy: 0.9842 - ignore_accuracy: 0.9469\n",
            "Epoch 10/100\n",
            "70/70 [==============================] - 119s 2s/step - loss: 0.0495 - accuracy: 0.9851 - ignore_accuracy: 0.9500\n",
            "Epoch 11/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0482 - accuracy: 0.9855 - ignore_accuracy: 0.9514\n",
            "Epoch 12/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0470 - accuracy: 0.9859 - ignore_accuracy: 0.9524\n",
            "Epoch 13/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0462 - accuracy: 0.9861 - ignore_accuracy: 0.9529\n",
            "Epoch 14/100\n",
            "70/70 [==============================] - 119s 2s/step - loss: 0.0453 - accuracy: 0.9862 - ignore_accuracy: 0.9532\n",
            "Epoch 15/100\n",
            "70/70 [==============================] - 119s 2s/step - loss: 0.0444 - accuracy: 0.9865 - ignore_accuracy: 0.9542\n",
            "Epoch 16/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0435 - accuracy: 0.9867 - ignore_accuracy: 0.9548\n",
            "Epoch 17/100\n",
            "70/70 [==============================] - 119s 2s/step - loss: 0.0427 - accuracy: 0.9869 - ignore_accuracy: 0.9554\n",
            "Epoch 18/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0419 - accuracy: 0.9871 - ignore_accuracy: 0.9560\n",
            "Epoch 19/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0408 - accuracy: 0.9874 - ignore_accuracy: 0.9571\n",
            "Epoch 20/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0399 - accuracy: 0.9876 - ignore_accuracy: 0.9576\n",
            "Epoch 21/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0392 - accuracy: 0.9878 - ignore_accuracy: 0.9584\n",
            "Epoch 22/100\n",
            "70/70 [==============================] - 119s 2s/step - loss: 0.0385 - accuracy: 0.9880 - ignore_accuracy: 0.9591\n",
            "Epoch 23/100\n",
            "70/70 [==============================] - 118s 2s/step - loss: 0.0377 - accuracy: 0.9883 - ignore_accuracy: 0.9598\n",
            "Epoch 24/100\n",
            "70/70 [==============================] - 120s 2s/step - loss: 0.0371 - accuracy: 0.9883 - ignore_accuracy: 0.9600\n",
            "Epoch 25/100\n",
            "70/70 [==============================] - 119s 2s/step - loss: 0.0363 - accuracy: 0.9886 - ignore_accuracy: 0.9610\n",
            "Epoch 26/100\n",
            "70/70 [==============================] - 119s 2s/step - loss: 0.0356 - accuracy: 0.9887 - ignore_accuracy: 0.9613\n",
            "Epoch 27/100\n",
            "70/70 [==============================] - 119s 2s/step - loss: 0.0348 - accuracy: 0.9889 - ignore_accuracy: 0.9618\n",
            "Epoch 28/100\n",
            "69/70 [============================>.] - ETA: 1s - loss: 0.0341 - accuracy: 0.9892 - ignore_accuracy: 0.9627"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-54d658af5cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImwnqdfMRb5S",
        "outputId": "127a8055-ed0f-40a6-e3aa-61394505dead",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def obt_test_data(file,word_num):\n",
        "\t# Opening file \n",
        "  f = open(file, 'r') \n",
        "  sents = []\n",
        "  sent = []\n",
        "  sents_tags = []\n",
        "  sent = [0]\n",
        "  sent_tags = [0]\n",
        "  len_sentences = []\n",
        "  len_sentence = 0\n",
        "  for line in f:\n",
        "    cur_line = line.strip()\n",
        "    if not cur_line:\n",
        "      sents.append(sent)\n",
        "      sents_tags.append(sent_tags)\n",
        "      sent = [0]\n",
        "      sent_tags = [0]\n",
        "      len_sentences.append(len_sentence)\n",
        "      len_sentence = 0\n",
        "    else:\n",
        "      cur_line_list = cur_line.split(\" \")\n",
        "      cur_line_list[0] = cur_line_list[0].lower()\n",
        "      cur_line_list[2] = cur_line_list[2][0]\n",
        "      key = tuple(cur_line_list[1])\n",
        "      if key not in word_num:\n",
        "        sent.append(1)\n",
        "      else:\n",
        "        sent.append(word_num[key])\n",
        "      len_sentence+=1\n",
        "      sent_tags.append(tag_to_index[cur_line_list[2]])\n",
        "  len_sentencs = np.asarray(len_sentences)\n",
        "  return sents, sents_tags, len_sentences\n",
        "test_sents, test_sents_tags,len_sentences = obt_test_data(os.path.join(path,\"test.txt\"), word_num)\n",
        "test_words, test_tags = pad_sentences(test_sents, test_sents_tags,MAX_LEN)\n",
        "test_tags_one_hot = np.asarray(one_hot(test_tags,3))\n",
        "model.evaluate(\n",
        "    test_words, test_tags_one_hot\n",
        "    )\n",
        "Y_pred = model.predict(\n",
        "    test_words, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10,\n",
        "    workers=1, use_multiprocessing=False\n",
        ")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 2 ... 0 0 0]\n",
            " [0 1 1 ... 0 0 0]\n",
            " [0 1 1 ... 0 0 0]\n",
            " ...\n",
            " [0 1 1 ... 0 0 0]\n",
            " [0 1 1 ... 0 0 0]\n",
            " [0 1 1 ... 0 0 0]]\n",
            "63/63 [==============================] - 10s 163ms/step - loss: 0.0396 - accuracy: 0.9877 - ignore_accuracy: 0.9592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrBzp3_scWvj"
      },
      "source": [
        "Y_pred = np.argmax(Y_pred,axis = 2)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJe5wbZMijEw",
        "outputId": "becf6051-cac5-4b81-8c53-a322358159fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "final_y_pred_mux = []\n",
        "final_y_true_mux = []\n",
        "assert len(len_sentences) == Y_pred.shape[0]\n",
        "for i in range(Y_pred.shape[0]):\n",
        "  final_y_pred_mux.append(Y_pred[i,1:1+len_sentences[i]])\n",
        "  final_y_true_mux.append(test_tags[i, 1:1+len_sentences[i]])\n",
        "\n",
        "final_y_pred = [val for el in final_y_pred_mux for val in el]\n",
        "final_y_true = [val for el in final_y_true_mux for val in el]\n",
        "\n",
        "\n",
        "Y_pred = Y_pred.flatten()\n",
        "test_tags = test_tags.flatten()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(final_y_true, final_y_pred))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5914   133   133]\n",
            " [   51 23214   587]\n",
            " [  228   824 16293]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VurM3bQpVv3N",
        "outputId": "127cf7b8-30d0-42d2-e39d-b128985e4adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(final_y_true, final_y_pred))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.96      6180\n",
            "           1       0.96      0.97      0.97     23852\n",
            "           2       0.96      0.94      0.95     17345\n",
            "\n",
            "    accuracy                           0.96     47377\n",
            "   macro avg       0.96      0.96      0.96     47377\n",
            "weighted avg       0.96      0.96      0.96     47377\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnGIcz2CsYFO",
        "outputId": "b8051655-4cc8-4a27-dbd5-cc5fb3357631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "conf_matr = confusion_matrix(test_tags, Y_pred)\n",
        "fig, ax = plt.subplots()\n",
        "#Since we ignore data originally from O\n",
        "conf_matr = confusion_matrix(final_y_true, final_y_pred)\n",
        "im = ax.imshow(conf_matr)\n",
        "\n",
        "ax.set_xticks(np.arange(len(tags)))\n",
        "ax.set_yticks(np.arange(len(tags)))\n",
        "# ... and label them with the respective list entries\n",
        "ax.set_xticklabels(tags)\n",
        "ax.set_yticklabels(tags)\n",
        "\n",
        "# # Rotate the tick labels and set their alignment.\n",
        "# plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "#          rotation_mode=\"anchor\")\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(tags)):\n",
        "    for j in range(len(tags)):\n",
        "        text = ax.text(j, i, conf_matr[i,j],\n",
        "                       ha=\"center\", va=\"center\", color=\"w\")\n",
        "\n",
        "ax.set_title(\"Heat Map\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8zVFzzkxdvN"
      },
      "source": [
        "model.save(os.path.join(path,\"only_pos.h5\"))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOb6ZZnT61x2"
      },
      "source": [
        "#Error Analysis Code (Please Ignore)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRB0OeM2oTXl",
        "outputId": "29aefc76-b8a9-47c9-c952-503dd0ebbdac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "count = 0\n",
        "wrong_sentences = []\n",
        "wrong_sentences_results = []\n",
        "for i in range(1000):\n",
        "  sentence= np.asarray([test_words[i]])\n",
        "  tags = np.asarray([test_tags_one_hot[i]])\n",
        "  result = model.evaluate(sentence, tags)\n",
        "  if result[-1] < 1:\n",
        "    wrong_sentences.append(i)\n",
        "    wrong_sentences_results.append(model.predict(sentence))\n",
        "    count+=1\n",
        "  if count > 60:\n",
        "    break\n",
        "print(wrong_sentences)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0109 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.9747 - ignore_accuracy: 0.9231\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0083 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9367 - ignore_accuracy: 0.7857\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0137 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0034 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.9622e-04 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0273 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0052 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0030 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0553 - accuracy: 0.9494 - ignore_accuracy: 0.8333\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0081 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.9873 - ignore_accuracy: 0.8750\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0047 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9873 - ignore_accuracy: 0.9375\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0031 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 0.9873 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9747 - ignore_accuracy: 0.8824\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0086 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9367 - ignore_accuracy: 0.9167\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0484 - accuracy: 0.9747 - ignore_accuracy: 0.8571\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.9873 - ignore_accuracy: 0.9800\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2486 - accuracy: 0.9241 - ignore_accuracy: 0.8649\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.9873 - ignore_accuracy: 0.9286\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0307 - accuracy: 0.9873 - ignore_accuracy: 0.9697\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0203 - accuracy: 0.9873 - ignore_accuracy: 0.9688\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 4.3019e-04 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0449 - accuracy: 0.9873 - ignore_accuracy: 0.9545\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0450 - accuracy: 0.9873 - ignore_accuracy: 0.9655\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0092 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0590 - accuracy: 0.9873 - ignore_accuracy: 0.9783\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0542 - accuracy: 0.9873 - ignore_accuracy: 0.9583\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0116 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9873 - ignore_accuracy: 0.9333\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0478 - accuracy: 0.9873 - ignore_accuracy: 0.8750\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.9620 - ignore_accuracy: 0.8421\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0067 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9873 - ignore_accuracy: 0.9600\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0466 - accuracy: 0.9873 - ignore_accuracy: 0.9375\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0393 - accuracy: 0.9873 - ignore_accuracy: 0.9412\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9747 - ignore_accuracy: 0.9048\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0033 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0296 - accuracy: 0.9873 - ignore_accuracy: 0.9545\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.9747 - ignore_accuracy: 0.8333\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9873 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9747 - ignore_accuracy: 0.9615\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8374e-04 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.9873 - ignore_accuracy: 0.9500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0480 - accuracy: 0.9747 - ignore_accuracy: 0.9333\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0063 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0176 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0034 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9367 - ignore_accuracy: 0.8571\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.9873 - ignore_accuracy: 0.9412\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.3295e-05 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.9747 - ignore_accuracy: 0.9615\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0409 - accuracy: 0.9747 - ignore_accuracy: 0.9459\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0288 - accuracy: 0.9873 - ignore_accuracy: 0.9545\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0284 - accuracy: 0.9873 - ignore_accuracy: 0.9706\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0087 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0247 - accuracy: 0.9873 - ignore_accuracy: 0.9500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0510 - accuracy: 0.9873 - ignore_accuracy: 0.9412\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0101 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0362 - accuracy: 0.9873 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9873 - ignore_accuracy: 0.9796\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0130 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0238 - accuracy: 0.9873 - ignore_accuracy: 0.8750\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0078 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0913 - accuracy: 0.9367 - ignore_accuracy: 0.8846\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0234 - accuracy: 0.9873 - ignore_accuracy: 0.9583\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9494 - ignore_accuracy: 0.8846\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0327 - accuracy: 0.9873 - ignore_accuracy: 0.9333\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0296 - accuracy: 0.9873 - ignore_accuracy: 0.9333\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9620 - ignore_accuracy: 0.9048\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 0.9873 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0847 - accuracy: 0.9873 - ignore_accuracy: 0.9630\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0032 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0103 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9620 - ignore_accuracy: 0.8889\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9873 - ignore_accuracy: 0.8889\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1683 - accuracy: 0.9494 - ignore_accuracy: 0.8000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3767 - accuracy: 0.9241 - ignore_accuracy: 0.8333\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0108 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0080 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0237 - accuracy: 0.9747 - ignore_accuracy: 0.9600\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0046 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9747 - ignore_accuracy: 0.9615\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0092 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9241 - ignore_accuracy: 0.9130\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9747 - ignore_accuracy: 0.9000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0100 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0417 - accuracy: 0.9873 - ignore_accuracy: 0.9787\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0133 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9873 - ignore_accuracy: 0.9706\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9620 - ignore_accuracy: 0.9286\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0177 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.9367 - ignore_accuracy: 0.9070\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 1.0000 - ignore_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0810 - accuracy: 0.9494 - ignore_accuracy: 0.7143\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.9367 - ignore_accuracy: 0.8788\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0533 - accuracy: 0.9747 - ignore_accuracy: 0.6000\n",
            "[2, 4, 12, 14, 16, 19, 21, 22, 24, 25, 26, 27, 32, 34, 35, 38, 39, 42, 43, 44, 47, 48, 49, 50, 52, 53, 55, 58, 59, 64, 65, 67, 68, 71, 72, 74, 75, 78, 81, 85, 86, 87, 88, 89, 90, 92, 95, 97, 98, 99, 102, 105, 107, 108, 111, 113, 114, 116, 118, 119, 120]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-111283d929ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrong_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0ms\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"str\") to tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf7lHVIApPcZ"
      },
      "source": [
        "def lines(file ):\n",
        "  count = 0\n",
        "\t# Opening file \n",
        "  f = open(file, 'r') \n",
        "  sents = []\n",
        "  sent = []\n",
        "  for line in f:\n",
        "    cur_line = line.strip()\n",
        "    if not cur_line:\n",
        "      sents.append(sent)\n",
        "      sent = []\n",
        "    else:\n",
        "      cur_line_list = cur_line.split(\" \")\n",
        "      cur_line_list[2] = cur_line_list[2][0]\n",
        "      sent.append(cur_line_list[0])\n",
        "  return sents\n",
        "      "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cjZujkktZ9t"
      },
      "source": [
        "def num_to_tags(nums):\n",
        "  labels = [\"O\", \"B\", \"I\"]\n",
        "  answer = []\n",
        "  for num in nums:\n",
        "    answer.append(labels[num])\n",
        "  return answer"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZolWnXZp6X9",
        "outputId": "5791813a-9e18-4035-8cee-549a5c0cc0b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "all_test_sents = lines(os.path.join(path,\"test.txt\"))\n",
        "print(num_to_tags(test_sents_tags[119]))\n",
        "print(num_to_tags((np.argmax(wrong_sentences_results[-2], axis = 2)).squeeze())[:len(all_test_sents[119])+2])\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'B', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'B', 'I', 'B', 'B', 'I', 'B', 'B', 'B', 'I', 'B', 'B', 'I', 'O']\n",
            "['O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'O', 'B', 'B', 'I', 'O', 'B', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'B', 'I', 'B', 'B', 'I', 'B', 'B', 'B', 'I', 'B', 'B', 'I', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM4pycj-scXC",
        "outputId": "22224686-d2a6-44ba-884f-b9e892b1c26b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(all_test_sents[11])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Is', 'such', 'a', 'view', 'justified', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}